{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "\n",
    "1. Add your name and HW Group Number below.\n",
    "2. Complete each question. Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", and delete and `throw NotImplementedError()` lines.\n",
    "3. Where applicable, run the test cases *below* each question to check your work. **Note**: In addition to the test cases you can see, the instructor may run additional test cases, including using *other datasets* to validate you code.\n",
    "4. Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All). You can also use the **Validate** button to run all test cases.\n",
    "5. Turn in your homework by going to the main screen in JupyterHub, clicking the Assignments menu, and submitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Name: Ben Layko\n",
    "HW Group Number: 5\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "715486c98f9d4b5d10bba3e96caa065f",
     "grade": false,
     "grade_id": "cell-1c68b78b2ae8c190",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### HW3 Problem 1: Linear Regression\n",
    "\n",
    "In this exercise, you will apply linear regression and Lasso regression methods to the dataset supplied to you and then compare their results to determine whether Lasso regression is needed for this dataset. Additionally, you will use sklearn's pipeline framework, which is so helpful when you have a sequence of transforms (e.g. normalization) and estimators (e.g. classifiers or regressors). \n",
    "\n",
    "**Dataset description**: You are provided a dataset with 20 variables. Variables $x1\\ -\\ x19$ refer to the independent variables, while variable $y$ is your dependent variable. Training data is stored in the file `/etc/data/regression-train.csv`.\n",
    "\n",
    "**Note**: TAs will use a test set to verify your solution. The format (independent variables $x1\\ -\\ x19$, dependent variable  $y$) will be same, but TAs' file may contain different number of data points than the split version from training set. Please ensure you take this into account, and do not hard code any dimensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "90b01647b3871abd3925293f73aacf9f",
     "grade": false,
     "grade_id": "cell-09b96daf1e0cf44e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Import necessary library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "13b9cc190cf5e8789c642db8469b0576",
     "grade": false,
     "grade_id": "cell-bb0404ec9da83a2b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "      <th>x11</th>\n",
       "      <th>x12</th>\n",
       "      <th>x13</th>\n",
       "      <th>x14</th>\n",
       "      <th>x15</th>\n",
       "      <th>x16</th>\n",
       "      <th>x17</th>\n",
       "      <th>x18</th>\n",
       "      <th>x19</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>508</td>\n",
       "      <td>44</td>\n",
       "      <td>60</td>\n",
       "      <td>718</td>\n",
       "      <td>42</td>\n",
       "      <td>234</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>52</td>\n",
       "      <td>8</td>\n",
       "      <td>216</td>\n",
       "      <td>0</td>\n",
       "      <td>1998</td>\n",
       "      <td>472</td>\n",
       "      <td>136</td>\n",
       "      <td>236</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>610.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1020</td>\n",
       "      <td>106</td>\n",
       "      <td>198</td>\n",
       "      <td>1620</td>\n",
       "      <td>126</td>\n",
       "      <td>680</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>112</td>\n",
       "      <td>104</td>\n",
       "      <td>36</td>\n",
       "      <td>614</td>\n",
       "      <td>0</td>\n",
       "      <td>5744</td>\n",
       "      <td>1642</td>\n",
       "      <td>294</td>\n",
       "      <td>348</td>\n",
       "      <td>14</td>\n",
       "      <td>20</td>\n",
       "      <td>2300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1118</td>\n",
       "      <td>146</td>\n",
       "      <td>828</td>\n",
       "      <td>704</td>\n",
       "      <td>32</td>\n",
       "      <td>698</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>96</td>\n",
       "      <td>122</td>\n",
       "      <td>18</td>\n",
       "      <td>842</td>\n",
       "      <td>0</td>\n",
       "      <td>6324</td>\n",
       "      <td>1748</td>\n",
       "      <td>282</td>\n",
       "      <td>718</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>1850.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>922</td>\n",
       "      <td>70</td>\n",
       "      <td>452</td>\n",
       "      <td>222</td>\n",
       "      <td>48</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>108</td>\n",
       "      <td>108</td>\n",
       "      <td>22</td>\n",
       "      <td>152</td>\n",
       "      <td>2</td>\n",
       "      <td>1360</td>\n",
       "      <td>320</td>\n",
       "      <td>224</td>\n",
       "      <td>98</td>\n",
       "      <td>4</td>\n",
       "      <td>36</td>\n",
       "      <td>270.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>526</td>\n",
       "      <td>60</td>\n",
       "      <td>294</td>\n",
       "      <td>162</td>\n",
       "      <td>18</td>\n",
       "      <td>164</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "      <td>46</td>\n",
       "      <td>8</td>\n",
       "      <td>166</td>\n",
       "      <td>0</td>\n",
       "      <td>1776</td>\n",
       "      <td>440</td>\n",
       "      <td>140</td>\n",
       "      <td>172</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>630</td>\n",
       "      <td>50</td>\n",
       "      <td>78</td>\n",
       "      <td>996</td>\n",
       "      <td>48</td>\n",
       "      <td>188</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>70</td>\n",
       "      <td>120</td>\n",
       "      <td>26</td>\n",
       "      <td>136</td>\n",
       "      <td>0</td>\n",
       "      <td>1260</td>\n",
       "      <td>302</td>\n",
       "      <td>152</td>\n",
       "      <td>110</td>\n",
       "      <td>6</td>\n",
       "      <td>26</td>\n",
       "      <td>310.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>330</td>\n",
       "      <td>32</td>\n",
       "      <td>38</td>\n",
       "      <td>664</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "      <td>36</td>\n",
       "      <td>2</td>\n",
       "      <td>392</td>\n",
       "      <td>88</td>\n",
       "      <td>78</td>\n",
       "      <td>36</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>1146</td>\n",
       "      <td>156</td>\n",
       "      <td>262</td>\n",
       "      <td>2628</td>\n",
       "      <td>194</td>\n",
       "      <td>840</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>170</td>\n",
       "      <td>120</td>\n",
       "      <td>24</td>\n",
       "      <td>940</td>\n",
       "      <td>0</td>\n",
       "      <td>6396</td>\n",
       "      <td>1714</td>\n",
       "      <td>288</td>\n",
       "      <td>664</td>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "      <td>1920.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>472</td>\n",
       "      <td>22</td>\n",
       "      <td>398</td>\n",
       "      <td>250</td>\n",
       "      <td>2</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>54</td>\n",
       "      <td>30</td>\n",
       "      <td>26</td>\n",
       "      <td>232</td>\n",
       "      <td>2</td>\n",
       "      <td>2230</td>\n",
       "      <td>540</td>\n",
       "      <td>112</td>\n",
       "      <td>114</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>460.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>1120</td>\n",
       "      <td>132</td>\n",
       "      <td>18</td>\n",
       "      <td>664</td>\n",
       "      <td>130</td>\n",
       "      <td>520</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>178</td>\n",
       "      <td>192</td>\n",
       "      <td>16</td>\n",
       "      <td>466</td>\n",
       "      <td>2</td>\n",
       "      <td>3578</td>\n",
       "      <td>940</td>\n",
       "      <td>322</td>\n",
       "      <td>310</td>\n",
       "      <td>8</td>\n",
       "      <td>52</td>\n",
       "      <td>1250.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>132 rows Ã— 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       x1   x2   x3    x4   x5   x6  x7  x8   x9  x10  x11  x12  x13   x14  \\\n",
       "0     508   44   60   718   42  234   0   0   56   52    8  216    0  1998   \n",
       "1    1020  106  198  1620  126  680   2   2  112  104   36  614    0  5744   \n",
       "2    1118  146  828   704   32  698   2   2   96  122   18  842    0  6324   \n",
       "3     922   70  452   222   48  150   0   0  108  108   22  152    2  1360   \n",
       "4     526   60  294   162   18  164   2   2   52   46    8  166    0  1776   \n",
       "..    ...  ...  ...   ...  ...  ...  ..  ..  ...  ...  ...  ...  ...   ...   \n",
       "127   630   50   78   996   48  188   2   2   70  120   26  136    0  1260   \n",
       "128   330   32   38   664    4   20   0   2   26   18    4   36    2   392   \n",
       "129  1146  156  262  2628  194  840   0   0  170  120   24  940    0  6396   \n",
       "130   472   22  398   250    2  128   0   0   54   30   26  232    2  2230   \n",
       "131  1120  132   18   664  130  520   2   2  178  192   16  466    2  3578   \n",
       "\n",
       "      x15  x16  x17  x18  x19       y  \n",
       "0     472  136  236   12    4   610.0  \n",
       "1    1642  294  348   14   20  2300.0  \n",
       "2    1748  282  718   16    4  1850.0  \n",
       "3     320  224   98    4   36   270.0  \n",
       "4     440  140  172    8    2   500.0  \n",
       "..    ...  ...  ...  ...  ...     ...  \n",
       "127   302  152  110    6   26   310.0  \n",
       "128    88   78   36    6    4   150.0  \n",
       "129  1714  288  664   16   18  1920.0  \n",
       "130   540  112  114    8    0   460.0  \n",
       "131   940  322  310    8   52  1250.0  \n",
       "\n",
       "[132 rows x 20 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read the data\n",
    "df = pd.read_csv(\"/etc/data/regression-train.csv\")\n",
    "X = df.iloc[:,:-1]\n",
    "y = df.iloc[:,-1]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6bbd04d5d8909161bad586100a42a92d",
     "grade": false,
     "grade_id": "cell-64336a26b130234f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Part 1: Linear Regression\n",
    "You will write code in the function `linear_regression_pipeline` to normalize and train simple linear regression using pipeline. For regression, it is particularly important to normalize our data before training the model, so we can better interpret our coefficients.\n",
    "\n",
    "Detailed instructions for implementation and allowed packages have been provided in the comments.\n",
    "\n",
    "Before your begin, read the documentation on sklearn's [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) and [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "73676d7e0dae62704608ed48c83a99a3",
     "grade": false,
     "grade_id": "linear_regression_pipeline",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def linear_regression_pipeline(): \n",
    "    # Perform linear regression\n",
    "    # No Input!\n",
    "\n",
    "    # Output:\n",
    "    # A pipeline that normalizes the data and performs simple linear regression\n",
    "  \n",
    "    # allowed packages: sklearn.preprocessing, sklearn.linear_model and sklearn.pipeline\n",
    "  \n",
    "    # Function hints: Read the documentation for the function Pipeline (link above)\n",
    "    \n",
    "    # write code for building a linear regression model using X, y\n",
    "    regression = LinearRegression(normalize = True, copy_X= True).fit(X, y)\n",
    "    pipe = Pipeline(steps=[('scaler', StandardScaler()), ('LR', regression)])\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('LR', LinearRegression(normalize=True))])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your pipeline should include a standard scaler, followed by a LR model\n",
    "pipe = linear_regression_pipeline()\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(normalize=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can get the individual pieces of your pipeline by name\n",
    "# Note: You should replace 'linear' with whatever name you gave the LR model\n",
    "pipe['LR']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ddd129de4242737d1d2bbc6c210fa106",
     "grade": false,
     "grade_id": "cell-7796449cb13ababf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Cross-validation has two widely-used applications: optimizing (tuning) hyperparameters and evaluating the performance of machine learning models. There are different ways to implement cross-validation for evaluating regression. In this homework, you are required to use the function `cross_validate()` since it allows us to pass multiple scoring metrics simultaneously. You need to use root mean square error (RMSE) and mean absolute error (MAE). You can find the list of scoring metrics names [here](https://scikit-learn.org/stable/modules/model_evaluation.html#common-cases-predefined-values).\n",
    "\n",
    "To get a better insight into the function [cross_validate()](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html) and the concept of [cross_validation](https://scikit-learn.org/stable/modules/cross_validation.html), take a look at links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3a498a1ad0907135c1a93edbdd5dbf44",
     "grade": false,
     "grade_id": "evaluate_model_cv",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "def evaluate_model_cv(clf, X, y):\n",
    "    # Inputs:\n",
    "    # clf: the classifier or pipeline\n",
    "    # X: independent attributes(19 variables, x1-x19)\n",
    "    # y: dependent variable (vector, continous type, y)\n",
    "    \n",
    "    # Output:\n",
    "    # A dictionary which contains the arrays related to scorers\n",
    "    \n",
    "    # allowed packages: sklearn.model_selection \n",
    "    \n",
    "    # IMPORTANT note: we will need train scores of your Cross Validation so set the argument \"return_train_score\" as True\n",
    "    \n",
    "    return cross_validate(clf, X, y, scoring=[\"neg_root_mean_squared_error\", \"neg_mean_absolute_error\"], return_train_score = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a9ee04bed2b2b48a261d567b8bc5b069",
     "grade": false,
     "grade_id": "cell-d25b65e0489489dd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Have you **noticed** that scores for RMSE and MAE are negative? Why has sklearn decided to provide a negative version of them as default?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'linear_regression_pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-efd9e9faa6c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Now test your model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model_cv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear_regression_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m print(f'The mean of CV training RMSE: \\n{-scores[\"train_neg_root_mean_squared_error\"].mean()}',\n",
      "\u001b[0;31mNameError\u001b[0m: name 'linear_regression_pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "# Now test your model\n",
    "\n",
    "scores = evaluate_model_cv(linear_regression_pipeline(), X, y)\n",
    "\n",
    "print(f'The mean of CV training RMSE: \\n{-scores[\"train_neg_root_mean_squared_error\"].mean()}',\n",
    "      f'with a standard deviation of {scores[\"train_neg_root_mean_squared_error\"].std()}')\n",
    "print()\n",
    "print(f'The mean of CV test RMSE: \\n{-scores[\"test_neg_root_mean_squared_error\"].mean()}',\n",
    "      f'with a standard deviation of {scores[\"test_neg_root_mean_squared_error\"].std()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "92d10fefb64db0ae0923fcf55d7783af",
     "grade": false,
     "grade_id": "cell-c0fdac08a871cd66",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You can access the parameters specific to the estimators of a pipeline by their index or name, which is useful in scenarios like below. To find out more, the documentation on [Pipelines and composite estimators](https://scikit-learn.org/stable/modules/compose.html) is helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model coefficients:\n",
      "[-4.85506628e-01  1.51072612e+02  1.86920788e+02  1.18356951e+02\n",
      "  4.34612148e+02 -3.69302925e+02  1.32149036e+02 -8.32898630e+01\n",
      "  3.65647936e+01  4.60077736e+01 -6.09234666e+01 -2.81553261e+02\n",
      " -1.34567899e+02 -2.46327897e+03  3.17584946e+03 -1.99829081e+02\n",
      "  8.85855284e+01 -1.07691888e+02  1.12269390e+02]\n"
     ]
    }
   ],
   "source": [
    "# Which attributes are most predictive of the outcome variable?\n",
    "\n",
    "# It is assumed that LinearRegression() is the second component of your pipeline that's why index 1 is called . \n",
    "# You can update the index if it has a different position in your implementation\n",
    "\n",
    "lr_model = linear_regression_pipeline().fit(X,y)\n",
    "print(f'Model coefficients:\\n{lr_model[1].coef_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9cbc879e309165782e8a13af756bd25d",
     "grade": true,
     "grade_id": "linear_regression_pipeline-public",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# We split dataset to training/testing sets ONLY for testing your pipeline\n",
    "# They are not needed anywhere else in this assignment\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=0)\n",
    "\n",
    "lr_model = linear_regression_pipeline().fit(X_train,y_train)\n",
    "simple_linear_regression_result = lr_model.predict(X_test)\n",
    "np.testing.assert_equal(simple_linear_regression_result.shape, (27,))\n",
    "np.testing.assert_almost_equal(evaluate_model_cv(linear_regression_pipeline(), X, y)['train_neg_mean_absolute_error'][2],-403.7440912543305)\n",
    "np.testing.assert_almost_equal(evaluate_model_cv(linear_regression_pipeline(), X, y)['test_neg_root_mean_squared_error'][1],-585.52899577)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0fce2185be2a72fcaad436d9193f70f3",
     "grade": true,
     "grade_id": "linear_regression_pipeline-private",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Remember, we will also use hidden test cases - you should create your own too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Lasso Regression\n",
    "You will write code in the function `lasso_regression_pipeline` to train simple lasso regression. Detailed instructions for implementation and allowed packages have been provided in the comments. \n",
    "\n",
    "Before your begin, read the documentation on sklearn's [LassoCV](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html) - a Lasso regression model that uses CV to tune its hyperparameters.\n",
    "\n",
    "**Note** that the lasso regression model has *built-in* crossvalidation, which it performs on the training dataset provided, to select the best shrinkage coefficient for the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5ba23c4bbc5d7bf2a4b2da2cd3ff7136",
     "grade": false,
     "grade_id": "lasso_regression_pipeline",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "def lasso_regression_pipeline(random_state=0):\n",
    "    # Inputs:\n",
    "    # random_state: a random state to use in CV model training\n",
    "    \n",
    "    # General Information:\n",
    "        # use 10-fold cross validation to determine the best model hyperparameters\n",
    "    \n",
    "    # Output:\n",
    "    # A pipeline which normalize the data and performs lasso regression\n",
    "\n",
    "  \n",
    "    # allowed packages: sklearn.linear_model\n",
    "  \n",
    "    # Function hints: Read the documentation for functions pipeline and LassoCV (links above)\n",
    "    \n",
    "    regression = LassoCV(cv = 10, random_state=random_state)\n",
    "    pipe = Pipeline(steps=[('scaler', StandardScaler()), ('Lasso', regression)])\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "da0bb49d44522a274e28988c130fd6a2",
     "grade": false,
     "grade_id": "cell-bcdac878e8d11144",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Before testing your model**: Do you expect the training error to be higher or lower? What about the testing error? What do you expect to be different about the coefficients?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean of CV training RMSE: \n",
      "557.210721315136 with a standard deviation of 86.90440293574433\n",
      "\n",
      "The mean of CV test RMSE: \n",
      "668.7583707472095 with a standard deviation of 236.36331461210918\n"
     ]
    }
   ],
   "source": [
    "# Now test your model\n",
    "# We can use the model evaluation function that we defined earlier\n",
    "\n",
    "scores = evaluate_model_cv(lasso_regression_pipeline(), X, y)\n",
    "\n",
    "print(f'The mean of CV training RMSE: \\n{-scores[\"train_neg_root_mean_squared_error\"].mean()}',\n",
    "      f'with a standard deviation of {scores[\"train_neg_root_mean_squared_error\"].std()}')\n",
    "print()\n",
    "print(f'The mean of CV test RMSE: \\n{-scores[\"test_neg_root_mean_squared_error\"].mean()}',\n",
    "      f'with a standard deviation of {scores[\"test_neg_root_mean_squared_error\"].std()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model coefficients:\n",
      "[  -0.          158.07311036   33.5272799    77.08926181  119.22238177\n",
      "   93.61157409   48.94047379    0.            0.            0.\n",
      "   -2.51163678    0.         -132.35067892    0.          343.58414203\n",
      "    0.            0.         -146.55285534   68.80125112]\n",
      "\n",
      "The shinkage coefficient hyperparameter chosen by CV: 14.052973668793825\n"
     ]
    }
   ],
   "source": [
    "# Which attributes are most predictive of the outcome variable?\n",
    "\n",
    "# It is assumed that LinearRegression() is the second component of your pipeline that's why index 1 is called . \n",
    "# You can update the index if it has a different position in your implementation\n",
    "\n",
    "lasso_model = lasso_regression_pipeline().fit(X, y)\n",
    "print(f'Model coefficients:\\n{lasso_model[1].coef_}')\n",
    "\n",
    "print()\n",
    "# Note we called this 'lamda' in class, but sklearn calls it alpha (should be ~14.05)\n",
    "print(f'The shinkage coefficient hyperparameter chosen by CV: {lasso_model[1].alpha_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "631720ce7ff726746c0f5eb04eae070b",
     "grade": true,
     "grade_id": "lasso_regression_pipeline-public",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# We split dataset to training/testing sets ONLY for testing your pipeline\n",
    "# They are not needed anywhere else in this assignment\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=0)\n",
    "\n",
    "lasso_model = lasso_regression_pipeline(0).fit(X_train, y_train)\n",
    "lasso_regression_result = lasso_model.predict(X_test)\n",
    "\n",
    "np.testing.assert_equal(lasso_regression_result.shape, (27,))\n",
    "np.testing.assert_almost_equal(evaluate_model_cv(lasso_regression_pipeline(0), X, y)['test_neg_root_mean_squared_error'][4],-583.27902826)\n",
    "np.testing.assert_almost_equal(evaluate_model_cv(lasso_regression_pipeline(0), X, y)['train_neg_mean_absolute_error'][0],-424.1614152)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "448ddd59fde4fcf118d2a3be30c7ec26",
     "grade": true,
     "grade_id": "lasso_regression_pipeline-private",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Remember, we will also use hidden test cases - you should create your own too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3d1c4a2efcc63aa0de1d4faa6713867b",
     "grade": false,
     "grade_id": "cell-59f0827753c683dc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "From the results, compare the two regression models, including the training and testing RMSE, and the coefficients. Use the output of these functions to answer the following questions below:\n",
    "\n",
    "1. The dataset contains 19 attributes. Are all 19 attributes useful for predicting the dependent variable? Why or why not? Use your results to justify the answer.\n",
    "2. If not all attributes are predictive, use your Lasso model to perform feature selection. Which attributes should be kept? Use a correlation and/or scatter plot to justify your answer for at least one attribute (in a new cell below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a908dd50efa01a0b6a490901e48b8c96",
     "grade": true,
     "grade_id": "comparison",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The two models performed close to the same, but were slightly different. The lasso regression had a higher training error (which is not abnormal), but did slightly better on the test data. The coefficients of the first regression model all seemed to have some sort of value (ranging from ~10 to ~3000 with one being smaller than 1). Lasso regression had 8 attributes that it reduced to such a small number it displayed as 0.\n",
    "1. Not all attributs were useful in prediction. One thing to note is that in the the Lasso regression, there were attributes with weights around 0, which means that they did not help in predicting the dependent variable.\n",
    "2. Any attribute that displayed as 0 could most likely be removed from lasso. These attributes either did not contribute much to the prediction of y, or were explained by other attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7ff6ddaa5630>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as mp\n",
    "x = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19] \n",
    "y = lasso_model[1].coef_\n",
    "\n",
    "plt.scatter(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remember**: Make sure to complete all problems (.ipynb files) in this assignment. When you finish, double-check the submission instructions at the top of this file, and submit on JupyterHub."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
